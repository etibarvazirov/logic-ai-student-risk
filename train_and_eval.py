# -*- coding: utf-8 -*-
"""train_and_eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BVMxwAx87kqykpDprpEpOkQ7pBR8Y-1p
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    f1_score
)
from sklearn.utils.class_weight import compute_class_weight

import torch
import torch.nn as nn
import torch.optim as optim

# ============================================================
# GLOBAL CONFIG
# ============================================================
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ============================================================
# MODEL DEFINITION
# ============================================================

class MLP(nn.Module):
    """
    A moderate MLP used for both the Standard NN and the LTN-like model.
    """
    def __init__(self, inp_dim, hidden=64, hidden2=32, dropout=0.2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(inp_dim, hidden),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden, hidden2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden2, 1),
            nn.Sigmoid()
        )
    def forward(self, x):
        return self.net(x).view(-1)

# ============================================================
# LOSS FUNCTIONS
# ============================================================

def make_weighted_bce(y_train):
    """
    Builds a class-weighted BCE loss function based on training distribution.
    We manually weight positive/negative examples.
    """
    classes = np.unique(y_train)
    class_w = compute_class_weight(
        class_weight='balanced',
        classes=classes,
        y=y_train
    )
    # map weights
    w_fail = class_w[classes.tolist().index(0)]
    w_pass = class_w[classes.tolist().index(1)]

    def weighted_bce(pred_proba, target):
        """
        pred_proba: (batch,) sigmoid output in [0,1]
        target    : (batch,) {0,1}
        """
        eps = 1e-7
        loss_vec = -( target * torch.log(pred_proba+eps) * w_pass
                    + (1-target) * torch.log(1-pred_proba+eps) * w_fail )
        return loss_vec.mean()
    return weighted_bce


class RuleModule:
    """
    Encodes domain rules for the LTN-like model.
    We implement two soft rules:

    Rule A (high-risk should NOT be predicted as Pass with high prob):
      IF (G1 low & G2 low & failures high & absences high)
      THEN model should not output high Pass prob.

    Rule B (strong student should NOT be predicted as Fail with high prob):
      IF (G1 high & G2 high & failures == 0 & studytime high)
      THEN model should not output low Pass prob.

    We'll penalize violations in a differentiable way:
      penaltyA ~ cond_A * pred_proba
      penaltyB ~ cond_B * (1 - pred_proba)
    """

    def __init__(self,
                 scaler,
                 numeric_cols,
                 low_G1_thr,
                 low_G2_thr,
                 high_abs_thr,
                 high_fail_thr,
                 high_G1_thr,
                 high_G2_thr,
                 high_study_thr,
                 device):
        self.scaler = scaler
        self.numeric_cols = numeric_cols
        self.device = device

        # store thresholds as tensors on device
        self.low_G1_thr_t   = torch.tensor(low_G1_thr, device=device)
        self.low_G2_thr_t   = torch.tensor(low_G2_thr, device=device)
        self.high_abs_thr_t = torch.tensor(high_abs_thr, device=device)
        self.high_fail_thr_t= torch.tensor(high_fail_thr, device=device)
        self.high_G1_thr_t  = torch.tensor(high_G1_thr, device=device)
        self.high_G2_thr_t  = torch.tensor(high_G2_thr, device=device)
        self.high_study_thr_t = torch.tensor(high_study_thr, device=device)

        # cache numeric column indices for readability
        self.G1_idx     = numeric_cols.index('G1')
        self.G2_idx     = numeric_cols.index('G2')
        self.fail_idx   = numeric_cols.index('failures')
        self.abs_idx    = numeric_cols.index('absences')
        self.study_idx  = numeric_cols.index('studytime')

        # torch versions of scaler params
        self.means_t = torch.tensor(self.scaler.mean_, device=device, dtype=torch.float32)
        self.stds_t  = torch.tensor(self.scaler.scale_, device=device, dtype=torch.float32)

    def __call__(self, x_batch, pred_proba):
        """
        x_batch: (batch, feat_dim) -- scaled inputs
        pred_proba: (batch,) sigmoid outputs

        returns: mean penalty from both rules
        """
        # unscale numeric features for rule checks
        x_num_scaled = x_batch[:, :len(self.numeric_cols)]
        x_num_unscaled = x_num_scaled * self.stds_t + self.means_t

        G1_val    = x_num_unscaled[:, self.G1_idx]
        G2_val    = x_num_unscaled[:, self.G2_idx]
        fail_v    = x_num_unscaled[:, self.fail_idx]
        abs_v     = x_num_unscaled[:, self.abs_idx]
        study_v   = x_num_unscaled[:, self.study_idx]

        # -------- Rule A: high-risk profile shouldn't be labelled Pass with high prob
        cond_A = (
            (G1_val < self.low_G1_thr_t) &
            (G2_val < self.low_G2_thr_t) &
            (fail_v >= self.high_fail_thr_t) &
            (abs_v  >= self.high_abs_thr_t)
        ).float()
        penaltyA = cond_A * pred_proba

        # -------- Rule B: strong student shouldn't be labelled Fail with high prob
        cond_B = (
            (G1_val > self.high_G1_thr_t) &
            (G2_val > self.high_G2_thr_t) &
            (fail_v <= 0.5) &  # basically no failures
            (study_v > self.high_study_thr_t)
        ).float()
        penaltyB = cond_B * (1.0 - pred_proba)

        loss_rules = penaltyA.mean() + penaltyB.mean()
        return loss_rules


# ============================================================
# METRICS, EVAL, THRESHOLD SEARCH
# ============================================================

def find_best_threshold(model, X_val_t, y_val_t):
    """
    We sweep thresholds and pick the one that maximizes F1 for the FAIL class (label=0).
    """
    model.eval()
    with torch.no_grad():
        probs = model(X_val_t).detach().cpu().numpy()

    best_thr = 0.5
    best_f1_fail = -1.0
    y_val_np = y_val_t.cpu().numpy()

    for thr in np.linspace(0.1, 0.9, 81):
        preds = (probs >= thr).astype(int)
        f1f = f1_score(y_val_np, preds, pos_label=0, zero_division=0)
        if f1f > best_f1_fail:
            best_f1_fail = f1f
            best_thr = thr
    return best_thr, best_f1_fail


def evaluate_model(model, X_t, y_t, threshold=0.5):
    """
    Returns accuracy, F1 for FAIL class, raw probs, and discrete preds.
    """
    model.eval()
    with torch.no_grad():
        probs = model(X_t).detach().cpu().numpy()
    preds = (probs >= threshold).astype(int)
    acc = accuracy_score(y_t.cpu().numpy(), preds)
    f1_fail = f1_score(y_t.cpu().numpy(), preds, pos_label=0)
    return acc, f1_fail, probs, preds


def rule_penalty_on_dataset(rule_module, X_t, probs_np):
    """
    For interpretability: how much each model violates rules on TEST.
    Lower is better (more rule-consistent).
    """
    with torch.no_grad():
        probs_t = torch.tensor(probs_np, device=device, dtype=torch.float32)
        penalty_val = rule_module(X_t, probs_t).item()
    return penalty_val


# ============================================================
# TRAINING LOOP (Standard NN / LTN-like)
# ============================================================

def train_model(model,
                X_train_t, y_train_t,
                X_val_t,   y_val_t,
                weighted_bce_fn,
                rule_module=None,
                lr=1e-3,
                max_epochs=200,
                patience=20):
    """
    If rule_module is None -> Standard NN.
    If rule_module is not None -> LTN-like model.
    """
    optimizer = optim.Adam(model.parameters(), lr=lr)

    best_val_score = None
    best_state = None
    epochs_no_improve = 0

    for epoch in range(max_epochs):
        model.train()
        optimizer.zero_grad()

        probs_train = model(X_train_t)   # (N,)
        bce_loss = weighted_bce_fn(probs_train, y_train_t.float())

        if rule_module is not None:
            # curriculum: lambda(t) grows over time
            lam = min(1.0, 0.1 + epoch / 100.0)  # starts ~0.1, approaches 1.0
            rule_loss = rule_module(X_train_t, probs_train)
            loss = bce_loss + lam * rule_loss
        else:
            loss = bce_loss

        loss.backward()
        optimizer.step()

        # ---- validation
        model.eval()
        with torch.no_grad():
            probs_val = model(X_val_t)
            bce_val = weighted_bce_fn(probs_val, y_val_t.float()).item()

        # we choose best checkpoint using FAIL F1 on validation
        thr_tmp, f1_fail_tmp = find_best_threshold(model, X_val_t, y_val_t)
        # combine -> first maximize f1_fail_tmp, break ties with lower bce_val
        val_score = (f1_fail_tmp, -bce_val)

        if (best_val_score is None) or (val_score > best_val_score):
            best_val_score = val_score
            best_state = {
                'model_state': model.state_dict(),
                'epoch': epoch,
                'thr': thr_tmp,
                'f1_fail_val': f1_fail_tmp,
                'bce_val': bce_val
            }
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        if epochs_no_improve > patience:
            break

    # load best
    model.load_state_dict(best_state['model_state'])
    return best_state


# ============================================================
# FULL PIPELINE
# ============================================================

def run_full_pipeline(csv_path="student-mat.csv"):
    """
    Loads data, trains Standard NN and LTN-like NN,
    evaluates on test set, and returns all results in a dict.
    """

    # ---------------------------
    # 1. LOAD DATA
    # ---------------------------
    df = pd.read_csv(csv_path, sep=';')

    # Binary label: Pass(1) vs Fail(0)
    # You can adjust threshold if your grading definition is different.
    df['target'] = (df['G3'] >= 10).astype(int)

    # Features
    numeric_cols = [
        'age','Medu','Fedu','traveltime','studytime','failures',
        'famrel','freetime','goout','Dalc','Walc','health',
        'absences','G1','G2'
    ]
    cat_cols = [
        'sex','address','schoolsup','famsup','paid','activities',
        'higher','internet','romantic'
    ]

    df_enc = pd.get_dummies(df[numeric_cols + cat_cols], drop_first=True)

    X_full = df_enc.values.astype(np.float32)
    y_full = df['target'].values.astype(np.int64)

    # ---------------------------
    # 2. TRAIN / VAL / TEST SPLIT (STRATIFIED)
    #    train 60%, val 20%, test 20%
    # ---------------------------
    X_temp, X_test, y_temp, y_test = train_test_split(
        X_full, y_full,
        test_size=0.2,
        stratify=y_full,
        random_state=SEED
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp,
        test_size=0.25,
        stratify=y_temp,
        random_state=SEED
    )
    # now: train 60%, val 20%, test 20%

    # ---------------------------
    # 3. SCALE NUMERIC COLS (fit only on TRAIN)
    # ---------------------------
    scaler = StandardScaler()
    X_train[:, :len(numeric_cols)] = scaler.fit_transform(
        X_train[:, :len(numeric_cols)]
    )
    X_val[:, :len(numeric_cols)]   = scaler.transform(
        X_val[:, :len(numeric_cols)]
    )
    X_test[:, :len(numeric_cols)]  = scaler.transform(
        X_test[:, :len(numeric_cols)]
    )

    # ---------------------------
    # 4. TORCH TENSORS
    # ---------------------------
    X_train_t = torch.tensor(X_train, device=device)
    y_train_t = torch.tensor(y_train, device=device)

    X_val_t   = torch.tensor(X_val, device=device)
    y_val_t   = torch.tensor(y_val, device=device)

    X_test_t  = torch.tensor(X_test, device=device)
    y_test_t  = torch.tensor(y_test, device=device)

    input_dim = X_train_t.shape[1]

    # ---------------------------
    # 5. BUILD WEIGHTED BCE LOSS
    # ---------------------------
    weighted_bce_fn = make_weighted_bce(y_train)

    # ---------------------------
    # 6. BUILD RULE MODULE
    #    thresholds from TRAIN (UNSCALED!)
    #    We'll recompute using unscaled train split to get percentiles.
    # ---------------------------
    # Recreate raw train before scaling for threshold calc
    X_temp_full, X_test_full, _, _ = train_test_split(
        X_full, y_full,
        test_size=0.2,
        stratify=y_full,
        random_state=SEED
    )
    X_train_full, X_val_full, _, _ = train_test_split(
        X_temp_full, y_temp,
        test_size=0.25,
        stratify=y_temp,
        random_state=SEED
    )
    train_numeric_unscaled = X_train_full[:, :len(numeric_cols)]

    # take percentiles
    G1_raw     = train_numeric_unscaled[:, numeric_cols.index('G1')]
    G2_raw     = train_numeric_unscaled[:, numeric_cols.index('G2')]
    fail_raw   = train_numeric_unscaled[:, numeric_cols.index('failures')]
    abs_raw    = train_numeric_unscaled[:, numeric_cols.index('absences')]
    study_raw  = train_numeric_unscaled[:, numeric_cols.index('studytime')]

    low_G1_thr     = np.percentile(G1_raw, 25)
    low_G2_thr     = np.percentile(G2_raw, 25)
    high_abs_thr   = np.percentile(abs_raw, 75)
    high_fail_thr  = np.percentile(fail_raw, 75)

    high_G1_thr    = np.percentile(G1_raw, 75)
    high_G2_thr    = np.percentile(G2_raw, 75)
    high_study_thr = np.percentile(study_raw, 75)

    rule_module = RuleModule(
        scaler=scaler,
        numeric_cols=numeric_cols,
        low_G1_thr=low_G1_thr,
        low_G2_thr=low_G2_thr,
        high_abs_thr=high_abs_thr,
        high_fail_thr=high_fail_thr,
        high_G1_thr=high_G1_thr,
        high_G2_thr=high_G2_thr,
        high_study_thr=high_study_thr,
        device=device
    )

    # ---------------------------
    # 7. TRAIN STANDARD NN
    # ---------------------------
    std_model = MLP(input_dim).to(device)
    best_std = train_model(
        std_model,
        X_train_t, y_train_t,
        X_val_t,   y_val_t,
        weighted_bce_fn,
        rule_module=None,       # no rules here
        lr=1e-3,
        max_epochs=200,
        patience=20
    )
    std_best_thr = best_std['thr']
    std_acc_test, std_f1_fail_test, std_probs_test, std_preds_test = evaluate_model(
        std_model, X_test_t, y_test_t, threshold=std_best_thr
    )
    cm_std = confusion_matrix(y_test_t.cpu().numpy(), std_preds_test)
    std_rule_pen = rule_penalty_on_dataset(rule_module, X_test_t, std_probs_test)

    # ---------------------------
    # 8. TRAIN LTN-LIKE MODEL
    # ---------------------------
    ltn_model = MLP(input_dim).to(device)
    best_ltn = train_model(
        ltn_model,
        X_train_t, y_train_t,
        X_val_t,   y_val_t,
        weighted_bce_fn,
        rule_module=rule_module, # rules ON
        lr=1e-3,
        max_epochs=200,
        patience=20
    )
    ltn_best_thr = best_ltn['thr']
    ltn_acc_test, ltn_f1_fail_test, ltn_probs_test, ltn_preds_test = evaluate_model(
        ltn_model, X_test_t, y_test_t, threshold=ltn_best_thr
    )
    cm_ltn = confusion_matrix(y_test_t.cpu().numpy(), ltn_preds_test)
    ltn_rule_pen = rule_penalty_on_dataset(rule_module, X_test_t, ltn_probs_test)

    # ---------------------------
    # 9. PACKAGE RESULTS
    # ---------------------------
    results = {
        "numeric_cols": numeric_cols,
        "cat_cols": cat_cols,
        "input_dim": input_dim,

        "std": {
            "best_epoch": best_std["epoch"],
            "threshold": float(std_best_thr),
            "accuracy": float(std_acc_test),
            "f1_fail": float(std_f1_fail_test),
            "cm": cm_std.tolist(),
            "rule_penalty": float(std_rule_pen),
        },
        "ltn": {
            "best_epoch": best_ltn["epoch"],
            "threshold": float(ltn_best_thr),
            "accuracy": float(ltn_acc_test),
            "f1_fail": float(ltn_f1_fail_test),
            "cm": cm_ltn.tolist(),
            "rule_penalty": float(ltn_rule_pen),
        }
    }

    # OPTIONAL: save trained LTN model so app.py can load it
    torch.save(ltn_model.state_dict(), "ltn_model.pt")
    torch.save(std_model.state_dict(), "std_model.pt")

    return results


# ============================================================
# MAIN (for local debugging / manual run)
# ============================================================
if __name__ == "__main__":
    out = run_full_pipeline(csv_path="student-mat.csv")

    print("=== Standard NN Results ===")
    print(f"Best epoch: {out['std']['best_epoch']}")
    print(f"Validation best threshold for FAIL F1: {out['std']['threshold']:.3f}")
    print(f"Test Accuracy: {out['std']['accuracy']:.4f}")
    print(f"Test F1(FAIL class): {out['std']['f1_fail']:.4f}")
    print(f"Confusion Matrix: {out['std']['cm']}")
    print(f"Rule Penalty: {out['std']['rule_penalty']:.6f}")

    print("\n=== LTN-like Model Results ===")
    print(f"Best epoch: {out['ltn']['best_epoch']}")
    print(f"Validation best threshold for FAIL F1: {out['ltn']['threshold']:.3f}")
    print(f"Test Accuracy: {out['ltn']['accuracy']:.4f}")
    print(f"Test F1(FAIL class): {out['ltn']['f1_fail']:.4f}")
    print(f"Confusion Matrix: {out['ltn']['cm']}")
    print(f"Rule Penalty: {out['ltn']['rule_penalty']:.6f}")