# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10_Hp9lIkZXdojCv81thlfOrVFcrG26hT
"""

import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import torch

from train_and_eval import run_full_pipeline

# -----------------------------------------------------------------
# 0. Page setup
# -----------------------------------------------------------------
st.set_page_config(
    page_title="LTN Student Performance Dashboard",
    page_icon="ðŸŽ“",
    layout="centered"
)

st.title("ðŸŽ“ Educational Outcome Prediction Dashboard")
st.write("""
This dashboard compares:
- a **Standard Neural Network**, and
- an **LTN-like model** (neural network + logic rules)

The goal is to detect students at risk of failing, in a way that is
both accurate and pedagogically interpretable.
""")

# -----------------------------------------------------------------
# 1. Run pipeline to get results
#    NOTE: This will train both models when the app starts.
#    Streamlit reruns on each interaction, so to avoid retraining
#    every time, we cache the results.
# -----------------------------------------------------------------

@st.cache_resource(show_spinner=True)
def get_results():
    # student-mat.csv must be in the same repo as app.py
    return run_full_pipeline(csv_path="student-mat.csv")

results = get_results()

# build std_results / ltn_results dicts so later code stays clean
std_results = {
    "label": "Standard NN",
    "accuracy": results["std"]["accuracy"],
    "f1_fail": results["std"]["f1_fail"],
    "threshold": results["std"]["threshold"],
    "cm": np.array(results["std"]["cm"]),
    "rule_penalty": results["std"]["rule_penalty"],
}

ltn_results = {
    "label": "LTN Model",
    "accuracy": results["ltn"]["accuracy"],
    "f1_fail": results["ltn"]["f1_fail"],
    "threshold": results["ltn"]["threshold"],
    "cm": np.array(results["ltn"]["cm"]),
    "rule_penalty": results["ltn"]["rule_penalty"],
}


# -----------------------------------------------------------------
# 2. Metric summary section
# -----------------------------------------------------------------
st.header("1. Model Summary Metrics")

col1, col2 = st.columns(2)

with col1:
    st.subheader(std_results["label"])
    st.metric("Accuracy", f"{std_results['accuracy']:.3f}")
    st.metric("F1 (FAIL class)", f"{std_results['f1_fail']:.3f}")
    st.metric("Best Decision Threshold", f"{std_results['threshold']:.3f}")
    st.metric("Rule Penalty (lower=better)", f"{std_results['rule_penalty']:.6f}")

with col2:
    st.subheader(ltn_results["label"])
    st.metric("Accuracy", f"{ltn_results['accuracy']:.3f}")
    st.metric("F1 (FAIL class)", f"{ltn_results['f1_fail']:.3f}")
    st.metric("Best Decision Threshold", f"{ltn_results['threshold']:.3f}")
    st.metric("Rule Penalty (lower=better)", f"{ltn_results['rule_penalty']:.6f}")

st.write("""
**Interpretation:**
The LTN model not only improves overall accuracy and F1 for the FAIL class,
but it also produces a lower rule penalty.
Lower rule penalty means the model's decisions are more aligned with
our educational rules (attendance, past failures, previous grades, etc.),
so it's a safer early-warning tool for teachers.
""")


# -----------------------------------------------------------------
# Helper plotting functions (matplotlib -> Streamlit)
# -----------------------------------------------------------------

def plot_confusion_matrix(cm, title='Confusion Matrix'):
    """
    cm is a 2x2 array in the format:
    [[TN, FP],
     [FN, TP]]
    Where class 0 = FAIL, class 1 = PASS.
    """
    fig, ax = plt.subplots(figsize=(3.5,3.5))
    im = ax.imshow(cm, cmap='Blues')

    ax.set_xticks([0,1])
    ax.set_yticks([0,1])
    ax.set_xticklabels(['Pred:Fail','Pred:Pass'])
    ax.set_yticklabels(['True:Fail','True:Pass'])
    ax.set_title(title)

    # annotate numbers
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, cm[i,j],
                    ha='center', va='center',
                    color='black', fontsize=11, fontweight='bold')

    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    fig.tight_layout()
    return fig


def plot_performance_bars(std_results, ltn_results):
    """
    Side-by-side bar plot for Accuracy and F1(FAIL).
    """
    labels = ['Accuracy', 'F1(FAIL)']
    std_vals = [std_results["accuracy"], std_results["f1_fail"]]
    ltn_vals = [ltn_results["accuracy"], ltn_results["f1_fail"]]

    x = np.arange(len(labels))
    width = 0.35

    fig, ax = plt.subplots(figsize=(4,3))
    ax.bar(x - width/2, std_vals, width, label=std_results["label"])
    ax.bar(x + width/2, ltn_vals, width, label=ltn_results["label"])

    ax.set_ylim(0,1)
    ax.set_ylabel('Score')
    ax.set_title('Performance Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.legend(loc='lower right')

    # write values on bars
    for i,v in enumerate(std_vals):
        ax.text(x[i]-width/2, v+0.01, f"{v:.2f}",
                ha='center', fontsize=9)
    for i,v in enumerate(ltn_vals):
        ax.text(x[i]+width/2, v+0.01, f"{v:.2f}",
                ha='center', fontsize=9)

    fig.tight_layout()
    return fig


def plot_rule_penalty(std_results, ltn_results):
    """
    Bar plot for rule penalty comparison.
    Lower = model follows our rules better.
    """
    labels = [std_results["label"], ltn_results["label"]]
    vals = [std_results["rule_penalty"], ltn_results["rule_penalty"]]

    fig, ax = plt.subplots(figsize=(4,3))
    ax.bar(labels, vals)
    ax.set_ylabel('Rule Penalty (lower = better)')
    ax.set_title('Rule Consistency')

    for i,v in enumerate(vals):
        # a little offset for the label text
        offset = v * 0.2 if v > 0 else 0.00005
        ax.text(i, v + offset,
                f"{v:.6f}",
                ha='center', fontsize=9)
    fig.tight_layout()
    return fig


# -----------------------------------------------------------------
# 3. Visual comparisons
# -----------------------------------------------------------------
st.header("2. Visual Comparison")

col3, col4 = st.columns(2)
with col3:
    st.subheader("Confusion Matrix: Standard NN")
    fig_std_cm = plot_confusion_matrix(std_results["cm"], "Standard NN")
    st.pyplot(fig_std_cm)

with col4:
    st.subheader("Confusion Matrix: LTN Model")
    fig_ltn_cm = plot_confusion_matrix(ltn_results["cm"], "LTN Model")
    st.pyplot(fig_ltn_cm)

st.subheader("Performance Metrics (Accuracy / F1(FAIL))")
fig_perf = plot_performance_bars(std_results, ltn_results)
st.pyplot(fig_perf)

st.subheader("Rule Consistency (Lower is Better)")
fig_rule = plot_rule_penalty(std_results, ltn_results)
st.pyplot(fig_rule)


# -----------------------------------------------------------------
# 4. Interactive "What if?" student profile demo
# -----------------------------------------------------------------
st.header("3. Try a Hypothetical Student")

st.write("""
Use the sliders to describe a hypothetical student, and interpret the risk.
In this demo version we explain how the rules would react.
(For a fully live predictor, weâ€™d load the trained model and run inference.)
""")

age = st.slider("Age", 15, 22, 17)
G1 = st.slider("G1 (first period grade)", 0, 20, 10)
G2 = st.slider("G2 (second period grade)", 0, 20, 10)
failures = st.slider("Number of past failures", 0, 4, 1)
absences = st.slider("Absences", 0, 50, 5)
studytime = st.slider("Study time category (1=low ..4=high)", 1, 4, 2)

st.write("### Interpretation of entered profile:")
high_risk_conditions = []
high_support_conditions = []

# Roughly mirror the rules:
# Rule A (risk): low G1/G2, high failures, high absences
# approx with heuristics since we don't have real percentiles here
if G1 < 8 and G2 < 8 and failures >= 2 and absences >= 15:
    high_risk_conditions.append(
        "Low recent grades + many past failures + many absences â†’ HIGH RISK of failing."
    )

# Rule B (support): high G1/G2, no failures, high studytime
if G1 >= 15 and G2 >= 15 and failures == 0 and studytime >= 3:
    high_support_conditions.append(
        "Strong grades, consistent study, no failures â†’ LOW RISK, likely to pass."
    )

if len(high_risk_conditions)==0 and len(high_support_conditions)==0:
    st.info("This profile is in a grey zone: neither clearly high-risk nor clearly safe. Closer monitoring may be needed.")
else:
    if len(high_risk_conditions)>0:
        for msg in high_risk_conditions:
            st.error(msg)
    if len(high_support_conditions)>0:
        for msg in high_support_conditions:
            st.success(msg)

st.write("""
**How this ties to LTN:**
The LTN model is explicitly penalized if it calls a clearly high-risk student "safe",
or calls a clearly strong student "at risk".
That is why the LTN model in this dashboard both performs well and respects
these educational rules.
""")

st.caption("All results are computed from the UCI Student Performance dataset.")